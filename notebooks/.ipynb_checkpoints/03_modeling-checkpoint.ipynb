{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae35d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We needa do our modelling while handling class imbalance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "df = pd.read_csv(\"../data/raw/WA_Fn-UseC_-Telco-Customer-Churn.csv\") #load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa3b7407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Distribution:\n",
      "   No Churn (0): 5,174 customers (73.5%)\n",
      "   Churn (1):    1,869 customers (26.5%)\n",
      "   Imbalance ratio: 2.8x more non-churners\n"
     ]
    }
   ],
   "source": [
    "#Prepare X-features and Y-Targets(needed for our train-test splitting later)\n",
    "\n",
    "X = df.drop('Churn', axis=1) # Features, everything except target\n",
    "Y = df['Churn'].map({'Yes': 1, 'No': 0}) # Target, encode as yes/no as 0/1\n",
    "\n",
    "print(f\"\\nClass Distribution:\")\n",
    "print(f\"   No Churn (0): {sum(Y == 0):,} customers ({np.mean(Y == 0):.1%})\") #shows sum and percentage of non churners VS churners\n",
    "print(f\"   Churn (1):    {sum(Y == 1):,} customers ({np.mean(Y == 1):.1%})\")\n",
    "print(f\"   Imbalance ratio: {sum(Y == 0)/sum(Y == 1):.1f}x more non-churners\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59ba519f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training set: 5,634 samples\n",
      "   Testing set:  1,409 samples\n",
      "   Training class ratio: 73.5% No churn\n",
      "   Testing class ratio:  73.5% No churn\n",
      "   Testing class ratio:  26.5% churn\n"
     ]
    }
   ],
   "source": [
    "# Split BEFORE any resampling to avoid data leakage! (called-Stratified Splitting)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, \n",
    "    test_size=0.2,        # 20% for testing, 80% for training\n",
    "    random_state=42,      #Using number 42 shuffle order\n",
    "    stratify=Y           # Keep the ratio of classes(non-churners & churners) the same in both sets\n",
    ")\n",
    "\n",
    "print(f\"   Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"   Testing set:  {X_test.shape[0]:,} samples\")\n",
    "print(f\"   Training class ratio: {np.mean(Y_train == 0):.1%} No churn\")\n",
    "print(f\"   Testing class ratio:  {np.mean(Y_test == 0):.1%} No churn\")\n",
    "print(f\"   Testing class ratio:  {np.mean(Y_test == 1):.1%} churn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "145ce55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to path: C:\\Users\\User\\Desktop\\Data Science Projects\\3)Telecome-churn-prediction proj\n",
      "Pipeline imported!\n"
     ]
    }
   ],
   "source": [
    "#Import our PREPROCESSING PIPELINE from src\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to Python path(so we dont get that \"src not defined\" error again like we did in '01_eda.ipyn')\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "\n",
    "sys.path.insert(0, project_root)\n",
    "print(f\"Added to path: {project_root}\")\n",
    "\n",
    "# Now import\n",
    "from src.preprocessing import create_preprocessing_pipeline\n",
    "print(\"Pipeline imported!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcd785ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing done!\n",
      "   Training shape: (5634, 41)\n",
      "   Testing shape:  (1409, 41)\n"
     ]
    }
   ],
   "source": [
    "#Use our preprocessing Pipeline\n",
    "preprocessor_pipeline = create_preprocessing_pipeline()\n",
    "\n",
    "# Fit ONLY on training data\n",
    "preprocessor_pipeline.fit(X_train, Y_train)\n",
    "\n",
    "#Transform X\n",
    "X_train_processed = preprocessor_pipeline.transform(X_train)\n",
    "X_test_processed = preprocessor_pipeline.transform(X_test)\n",
    "\n",
    "print(f\"Preprocessing done!\")\n",
    "print(f\"   Training shape: {X_train_processed.shape}\")\n",
    "print(f\"   Testing shape:  {X_test_processed.shape}\")\n",
    "\n",
    "#NOW u can easily try multiple models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e3afdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODELLING STARTS FROM HERE:\n",
    "#Once we find our perfect model to use then we can create our FINAL PIPELINE that will do the preprocessing+modelling=Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a91aeaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Churn       0.90      0.72      0.80      1035\n",
      "       Churn       0.50      0.78      0.61       374\n",
      "\n",
      "    accuracy                           0.74      1409\n",
      "   macro avg       0.70      0.75      0.71      1409\n",
      "weighted avg       0.80      0.74      0.75      1409\n",
      "\n",
      "ROC-AUC Score: 0.842\n"
     ]
    }
   ],
   "source": [
    "# TRAIN LOGISTIC REGRESSION MODEL(WITH CLASS WEIGHTS TO HANDLE IMBALANCE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "#Create model with class weights to handle imbalance\n",
    "lr_model = LogisticRegression(\n",
    "    class_weight='balanced',  # CRITICAL for imbalance!\n",
    "    random_state=42,\n",
    "    max_iter=1000            # Ensure convergence\n",
    ")\n",
    "\n",
    "\n",
    "lr_model.fit(X_train_processed, Y_train)\n",
    "\n",
    "Y_pred_lr= lr_model.predict(X_test_processed)\n",
    "Y_pred_proba_lr = lr_model.predict_proba(X_test_processed)[:, 1]  # Probability of churn thats why used 1 bec 1=churn\n",
    "\n",
    "\n",
    "#Evaluate the model\n",
    "print(classification_report(Y_test, Y_pred_lr, target_names=['No Churn', 'Churn']))\n",
    "\n",
    "\n",
    "roc_auc_lr = roc_auc_score(Y_test, Y_pred_proba_lr)\n",
    "print(f\"ROC-AUC Score: {roc_auc_lr:.3f}\")  #That .3f just means show 3 decimal places\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "167496fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#insights from our classification report and roc_auc_score\n",
    "#Recall = 0.78: You're catching 78% of actual churners! This is EXCELLENT!\n",
    "#(Missing only 22% of churners)\n",
    "\n",
    "#Precision = 0.50: When you flag someone as \"will churn\", you're right 50% of the time\n",
    "#(Half are false alarms - but that's OK for churn prediction!)\n",
    "\n",
    "#High precision (0.90): When you say \"won't churn\", you're right 90% of the time\n",
    "#Lower recall (0.72): incorrectly flagging 28% of loyal customers as \"risky\"\n",
    "#Low precision (0.50): When you say \"churn\", you're right 50% of the time\n",
    "#Higher recall (0.78): incorrectly flagging 22% of unloyal customers as \"not risky\"\n",
    "\n",
    "#ROC-AUC = 0.842 (thats very good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5459e4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Churn       0.88      0.79      0.83      1035\n",
      "       Churn       0.55      0.71      0.62       374\n",
      "\n",
      "    accuracy                           0.77      1409\n",
      "   macro avg       0.72      0.75      0.73      1409\n",
      "weighted avg       0.79      0.77      0.78      1409\n",
      "\n",
      "ROC-AUC Score: 0.837\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create Random Forest with weights\n",
    "rf_model = RandomForestClassifier(\n",
    "    class_weight = 'balanced',\n",
    "    random_state = 42,\n",
    "    n_estimators = 100,\n",
    "    max_depth = 10,   #\n",
    "    n_jobs = -1     #Use all CPU cores, so it'll train the model faster\n",
    ")\n",
    "\n",
    "#Fit the model\n",
    "rf_model.fit(X_train_processed, Y_train)\n",
    "\n",
    "#Predict\n",
    "Y_pred_rf = rf_model.predict(X_test_processed)\n",
    "Y_pred_proba_rf = rf_model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "\n",
    "#Evaluate the model\n",
    "print(classification_report(Y_test, Y_pred_rf, target_names=['No Churn', 'Churn']))\n",
    "\n",
    "roc_auc_rf = roc_auc_score(Y_test, Y_pred_proba_rf)\n",
    "print(f\"ROC-AUC Score: {roc_auc_rf:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9274e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression is BETTER at:\n",
    "#REM: Recall is how many churners are we actually catching?\n",
    "#Catching more churners (78% recall vs 71% recall) â†’ Saves more revenue!\n",
    "#Overall predictive skill (0.842 vs 0.837 ROC-AUC)\n",
    "\n",
    "#Random Forest is BETTER at:\n",
    "#Fewer false alarms (55% vs 50% wrong flags)\n",
    "#Slightly better balanced performance (0.62 vs 0.61 F1)\n",
    "\n",
    "#Business Wise\n",
    "#Remember tho it costs less to save existing customers than too get new customers\n",
    "#Therefore we will choose the Logistic Regression model\n",
    "#Also log_reg model is easier to explain WHY a customer might churn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "406649c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'track_experiment' from 'mlflow_tracking' (C:\\Users\\User\\Desktop\\Data Science Projects\\3)Telecome-churn-prediction proj\\notebooks\\mlflow_tracking.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#OKAY WE GOING TO ADD OUR MLFLOW CODE HERE(Just after trainig the models):\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# MLFLOW: TRACK AND COMPARE OUR MODELS\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Import our tracking functions that we created in our 'mlflow_tracking.py'\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlflow_tracking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m track_experiment, compare_models, show_mlflow_instructions\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Method 1: Track models ONE BY ONE\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m METHOD 1: Track Logistic Regression...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'track_experiment' from 'mlflow_tracking' (C:\\Users\\User\\Desktop\\Data Science Projects\\3)Telecome-churn-prediction proj\\notebooks\\mlflow_tracking.py)"
     ]
    }
   ],
   "source": [
    "#OKAY WE GOING TO ADD OUR MLFLOW CODE HERE(Just after trainig the models):\n",
    "\n",
    "# MLFLOW: TRACK AND COMPARE OUR MODELS\n",
    "\n",
    "# Import our tracking functions that we created in our 'mlflow_tracking.py'\n",
    "from mlflow_tracking import track_experiment, compare_models, show_mlflow_instructions\n",
    "\n",
    "# Method 1: Track models ONE BY ONE\n",
    "print(\"\\n METHOD 1: Track Logistic Regression...\")\n",
    "lr_params = {\n",
    "    'model': 'LogisticRegression',\n",
    "    'class_weight': 'balanced',\n",
    "    'max_iter': 1000,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "lr_tracked, lr_metrics = track_experiment(\n",
    "    lr_model, \n",
    "    'Logistic_Regression_Churn',\n",
    "    X_train_processed, X_test_processed, \n",
    "    Y_train, Y_test,\n",
    "    lr_params\n",
    ")\n",
    "\n",
    "print(\"\\n METHOD 1: Track Random Forest...\")\n",
    "rf_params = {\n",
    "    'model': 'RandomForest',\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 10,\n",
    "    'class_weight': 'balanced',\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "rf_tracked, rf_metrics = track_experiment(\n",
    "    rf_model,\n",
    "    'Random_Forest_Churn',\n",
    "    X_train_processed, X_test_processed,\n",
    "    Y_train, Y_test,\n",
    "    rf_params\n",
    ")\n",
    "\n",
    "\n",
    "# Method 2: OR compare both at once\n",
    "print(\"\\n METHOD 2: Compare both models at once...\")\n",
    "models_to_compare = {\n",
    "    'Logistic_Regression': lr_model,\n",
    "    'Random_Forest': rf_model\n",
    "}\n",
    "\n",
    "comparison_results = compare_models(\n",
    "    models_to_compare,\n",
    "    X_train_processed, X_test_processed,\n",
    "    Y_train, Y_test\n",
    ")\n",
    "\n",
    "# Show how to view results\n",
    "show_mlflow_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ab4eef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python path: ['C:\\\\Users\\\\User\\\\Desktop\\\\Data Science Projects\\\\3)Telecome-churn-prediction proj', 'C:\\\\Users\\\\User\\\\Desktop\\\\Data Science Projects\\\\3)Telecome-churn-prediction proj\\\\notebooks', 'C:\\\\Users\\\\User\\\\anaconda3\\\\python311.zip', 'C:\\\\Users\\\\User\\\\anaconda3\\\\DLLs', 'C:\\\\Users\\\\User\\\\anaconda3\\\\Lib', 'C:\\\\Users\\\\User\\\\anaconda3', '', 'C:\\\\Users\\\\User\\\\anaconda3\\\\Lib\\\\site-packages', 'C:\\\\Users\\\\User\\\\anaconda3\\\\Lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\User\\\\anaconda3\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\User\\\\anaconda3\\\\Lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:149: UserWarning: Field \"model_name\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions in module: ['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'compare_models', 'mlflow', 'pd', 'precision_score', 'recall_score', 'roc_auc_score', 'show_mlflow_instructions', 'track_experiment']\n"
     ]
    }
   ],
   "source": [
    "# Debug import\n",
    "import sys\n",
    "print(f\"Python path: {sys.path}\")\n",
    "\n",
    "# Try absolute import\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\n",
    "    \"mlflow_tracking\", \n",
    "    r\"C:\\Users\\User\\Desktop\\Data Science Projects\\3)Telecome-churn-prediction proj\\notebooks\\mlflow_tracking.py\"\n",
    ")\n",
    "module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(module)\n",
    "\n",
    "print(f\"Functions in module: {dir(module)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae40650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets create a completed Pipeline = preprocessing + model\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline as SKPipeline\n",
    "\n",
    "full_pipeline = SKPipeline([\n",
    "    ('preprocessing', preprocessor_pipeline),  # Your existing preprocessing\n",
    "    ('classifier', lr_model)                    # Your trained model\n",
    "])\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# NOW save the pipeline\n",
    "#IMP!!= (This code helped save our pipeline where it suppose to be saved and not in the 'notebooks/'' folder location YAY)\n",
    "import os\n",
    "\n",
    "# Create models folder in project root (not in notebooks/)\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save with ../ to go up from notebooks/ to project root\n",
    "model_path = '../models/churn_pipeline.pkl'\n",
    "joblib.dump(full_pipeline, model_path)\n",
    "\n",
    "print(f\" Model saved to: {os.path.abspath(model_path)}\")\n",
    "print(f\"File size: {os.path.getsize(model_path):,} bytes\") #If shows 0 bytes then something not right\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Test our pipeline\n",
    "test_customer = X_test.iloc[[0]]  # First test customer\n",
    "prediction = full_pipeline.predict(test_customer)\n",
    "probability = full_pipeline.predict_proba(test_customer)[0, 1]  #Do row 1 customer, get probability if churn(churn=1)\n",
    "\n",
    "print(f\"Test prediction: {'Churn' if prediction[0] == 1 else 'No Churn'}\")\n",
    "print(f\"Probability: {probability:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df516c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets try reload the full_pipeline\n",
    "loaded_pipeline = joblib.load('../models/churn_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a2d63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
